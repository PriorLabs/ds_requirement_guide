{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code skeleton example for PriorLabs Fine-Tuning Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Callable, Dict, List, Tuple\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import  benchmark_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ§ª  Minimal Example Implementation (synthetic data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_processing.base_datamodule import BaseDataModule\n",
    "\n",
    "class ExampleDataModule(BaseDataModule):\n",
    "    \"\"\"Synthetic placeholder so the skeleton runs outâ€‘ofâ€‘theâ€‘box.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_datasets: int = 3,\n",
    "        n_samples: int = 5000,\n",
    "        n_features: int = 100,\n",
    "        random_seed: int = 42,\n",
    "    ):\n",
    "        super().__init__(random_seed)\n",
    "        self.n_datasets = n_datasets\n",
    "        self.n_samples = n_samples\n",
    "        self.n_features = n_features\n",
    "\n",
    "    # ğŸš§  Replace `_make_dataset` + the *_datasets methods with real logic\n",
    "    def _make_dataset(self, rng) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        X = rng.normal(size=(self.n_samples, self.n_features))\n",
    "        y = (X[:, 0] + rng.normal(scale=0.5, size=self.n_samples) > 0).astype(int)\n",
    "        return X, y\n",
    "\n",
    "    def _generate(self) -> Tuple[List[np.ndarray], List[np.ndarray]]:\n",
    "        rng = np.random.default_rng(self.random_seed)\n",
    "        X_list, y_list = zip(*(self._make_dataset(rng) for _ in range(self.n_datasets)))\n",
    "        return list(X_list), list(y_list)\n",
    "\n",
    "    def train_datasets(self):\n",
    "        return self._generate()\n",
    "\n",
    "    def val_datasets(self):\n",
    "        # Optional: supply explicit validation sets or just return empty lists.\n",
    "        # We will sample from the training datasets in case None are provided!\n",
    "        return [], []\n",
    "\n",
    "    def test_datasets(self):\n",
    "        return self._generate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸš€  Quick sanity check: classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Datasets: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean scores across synthetic datasets:\n",
      "  MetricType.ACCURACY: 0.8467\n",
      "  MetricType.ROC_AUC: 0.9276\n",
      "  MetricType.F1: 0.8466\n",
      "  MetricType.LOG_LOSS: 0.5026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dm = ExampleDataModule()\n",
    "X_train_list, y_train_list = dm.train_datasets()\n",
    "\n",
    "model = xgb.XGBClassifier(\n",
    "    eval_metric=\"logloss\", random_state=42, n_estimators=200,\n",
    ")\n",
    "\n",
    "scores = benchmark_datasets(model, X_train_list, y_train_list)\n",
    "print(\"Mean scores across synthetic datasets:\")\n",
    "for name, val in scores.items():\n",
    "    print(f\"  {name:8s}: {val:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸš€  Quick sanity check: regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Datasets:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Datasets: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean scores across synthetic datasets:\n",
      "  MetricType.RMSE: 0.3192\n",
      "  MetricType.MSE: 0.1019\n",
      "  MetricType.MAE: 0.2056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dm = ExampleDataModule()\n",
    "X_train_list, y_train_list = dm.train_datasets()\n",
    "\n",
    "model = xgb.XGBRFRegressor(\n",
    "    eval_metric=\"logloss\", random_state=42, n_estimators=200,\n",
    ")\n",
    "\n",
    "scores = benchmark_datasets(model, X_train_list, y_train_list)\n",
    "print(\"Mean scores across synthetic datasets:\")\n",
    "for name, val in scores.items():\n",
    "    print(f\"  {name:8s}: {val:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ft4_py311_env, micromamba)",
   "language": "python",
   "name": "ft4_py311_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
